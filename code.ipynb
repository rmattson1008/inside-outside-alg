{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "R. Mattson\n",
    "\n",
    "\n",
    "Tuesday, Dec. 14 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO:\n",
    "\n",
    "Are my binary rules well formed? \n",
    "\n",
    "Does chart build up to Z sufficiently? I don't think so...?\n",
    "\n",
    "does it propagate well...\n",
    "\n",
    "What is the sum of other things??? \n",
    "\n",
    "WE should only care about S on the \n",
    "\n",
    "??LIKELIHOOD - review probability shit and put it on a slide\n",
    "\n",
    "I want \n",
    "\n",
    "rule building -> chart building -> inside-outside indices ->  E step gives likelihood -> whats sum of other counts? -> M step correctly calculates thetas -> em exits in a timely manner -> save g, its the core of PCFG.\n",
    "\n",
    "I think I could use a heuristic for avg_expected_counts... but if its not likelihood, we will run into problems later... \n",
    "e_step(sents, rules, G_i)\n",
    "and we want vector as long as our authors, and we say pred_i  = argmax(vector)\n",
    "\n",
    "Choose to progress on this or do presentation\n",
    "\n",
    "Goals:\n",
    "\n",
    "- make presentation slides\n",
    "- get inside-outside working well enough to get model trained for each author\n",
    "- compare likelihood of expectation step on test data\n",
    "- compare PCFG prediction performance to some simple models, eg naive bayes + BOW\n",
    "\n",
    "\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD INITIAL RULES AND WORDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path_to_text):\n",
    "    \n",
    "    words = np.loadtxt(path_to_text, delimiter=\"/n\", dtype=str,)\n",
    "    \n",
    "    sents = [[sent.split()] for sent in words]\n",
    "    sents = [[sent] for sent in sents if len(sent) > 2] # just a check\n",
    "   \n",
    "    return words\n",
    "rules_d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules_from_pcfg(path_to_pcfg):\n",
    "    rules = np.loadtxt(path_to_pcfg, delimiter=\"/n\", dtype=str)\n",
    "    binary_rule_length = len(rules[0].split())\n",
    "    rules = np.unique(rules)\n",
    "\n",
    "    binary_rules = []\n",
    "    unary_rules = []\n",
    "    nts = []\n",
    "    \n",
    "    for rule in rules:\n",
    "        rule = rule.split()\n",
    "        \n",
    "        if rule[0] not in nts:\n",
    "            nts.append(rule[0])\n",
    "        if len(rule) == binary_rule_length:\n",
    "            # print(rule)\n",
    "            prob = rand.random()\n",
    "            rule = np.array([rule[0], rule[2], rule[3], prob])\n",
    "            binary_rules.append(rule)\n",
    "\n",
    "        elif len(rule) < binary_rule_length:\n",
    "            prob = rand.random()\n",
    "            rule = np.array([rule[0], rule[2], prob])\n",
    "            unary_rules.append(rule)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            print(rule)\n",
    "    \n",
    "\n",
    "    \n",
    "    binary_rules = np.array(binary_rules) \n",
    "    unary_rules = np.array(unary_rules) \n",
    "    return unary_rules, binary_rules, nts\n",
    "    \n",
    "unary_rules, binary_rules, nts = load_rules_from_pcfg(\"../make-data/pcfg_ug.txt\")\n",
    "# binary_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside-Outside Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chart():\n",
    "    def __init__(self, nts, nts2idx, unary_rules, binary_rules, words):\n",
    "        self.n = len(words)\n",
    "        self.v = len(nts)\n",
    "        self.nts = nts\n",
    "        self.words = words\n",
    "        self.nts2idx = nts2idx\n",
    "        # self.nts2idx = nts2idx\n",
    "        self.arr = np.zeros([self.n, self.n +1, self.v])\n",
    "\n",
    "\n",
    "    def set(self, i, j, A, value):\n",
    "        rule_idx = self.nts2idx[A]\n",
    "        self.arr[i, j, rule_idx] = value\n",
    "        return\n",
    "\n",
    "    def get(self, i, j, A):\n",
    "        rule_idx = self.nts2idx[A]\n",
    "        return self.arr[i, j, rule_idx]\n",
    "  \n",
    "\n",
    "    def add(self, i, j, A, value):\n",
    "        rule_idx = self.nts2idx[A]\n",
    "        self.arr[i, j, rule_idx] += value\n",
    "        return \n",
    "\n",
    "    def print(self, num_slices=1):\n",
    "        print(\"shape:\", self.arr.shape)\n",
    "        if num_slices <= 0:\n",
    "            num_slices = self.v\n",
    "        # for r in range(num_slices):\n",
    "            # print(self.nts[r])\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n+1):\n",
    "                if i == j:\n",
    "                    # print(self.words[i], end=\"| \")\n",
    "                    print('{:.6s}'.format(self.words[i]), end=\"| \")\n",
    "                    #  print(self.words[i].center(4), end= \"| \")\n",
    "                else:\n",
    "                    # print('{:.6f}'.format(self.arr[i, j, r]), end=\"| \")\n",
    "                    assert len(self.arr[i, j]) == self.v\n",
    "                    max_value = np.max(self.arr[i, j])\n",
    "                    if max_value > 0.0:\n",
    "                        print('{:.6f}'.format(max_value), end=\"| \")\n",
    "                    else: \n",
    "                        print(\"000000\", end=\"| \")\n",
    "            print()\n",
    "\n",
    "    \n",
    "\n",
    "# lmao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inside(sent, params=[]):\n",
    "def inside(words, unary_rules, binary_rules, nts, nts2idx, g):\n",
    "    # print(\"ENTERING INSIDE\")\n",
    "    # print(type(unary_rules), type(binary_rules))\n",
    "\n",
    "    chart = []\n",
    "    # words = sent.split(\" \")\n",
    "    # print(words)\n",
    "    n = len(words)\n",
    "    num_unary_rules_used = 0\n",
    "    chart = Chart(nts, nts2idx, unary_rules, binary_rules, words)\n",
    "\n",
    "    # TODO- where is good?\n",
    "    # print(\"pretty\" in words)\n",
    "    unary = [x for x in unary_rules if x[1] in words]\n",
    "\n",
    "    v = max(len(unary), len(binary_rules))\n",
    "    substitute_rule = rand.choice(unary_rules)\n",
    "\n",
    "    for k in range(n):\n",
    "        # print(\"word\", words[k])\n",
    "        relevant_rules = [x for x in unary if words[k] in x]\n",
    "        # print(rules)\n",
    "        # print(\"Chart span\", k, k+1)\n",
    "        if not relevant_rules:\n",
    "            print(\"no unary rule found for \", words[k])\n",
    "            prob = g[substitute_rule]\n",
    "            chart.set(k, k+1, substitute_rule[0], value=prob)\n",
    "        for rule in relevant_rules:\n",
    "            prob = g[rule]\n",
    "            chart.set(k, k+1, rule[0], value=prob)\n",
    "\n",
    "\n",
    "    examined = []\n",
    "    for sub_string_length in range(2, n+1):\n",
    "        # print(\"substring length\", sub_string_length)\n",
    "        # print(\"L\", l)\n",
    "        for start_idx in range(n-sub_string_length+1):\n",
    "            # start_idx +=1\n",
    "            # end_idx = start_idx + sub_string_length+1\n",
    "            end_idx = start_idx + sub_string_length\n",
    "            # print(\"Chart span\", start_idx, end_idx)\n",
    "            for mid_idx in range(start_idx +1 , end_idx):  # IDX???????\n",
    "                # print(\"subtrees:\", start_idx, mid_idx, end_idx)\n",
    "                examined.append((start_idx, end_idx))\n",
    "                for rule in binary_rules:\n",
    "                    A, B, C = rule\n",
    "                    # print(rule)\n",
    "                    # g = rule[-1].astype(np.float)\n",
    "                    new_prob = g[rule] * chart.get(start_idx, mid_idx, B) * chart.get(mid_idx, end_idx, C)\n",
    "                    # new_prob = chart.get(start_idx, mid_idx, B) * chart.get(mid_idx, end_idx, C)\n",
    "                    chart.add(start_idx, end_idx, A, value=new_prob)\n",
    "            \n",
    "\n",
    "\n",
    "                  \n",
    "    return chart  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def outside(sent, unary_rules, binary_rules, nts, nts2idx, g):\n",
    "    # print(\"ENTERING OUTSIDE\")\n",
    "    words = sent.split(\" \")\n",
    "    # # words = sent\n",
    "    # if len(words) > 5:\n",
    "    #     words = words[:5]\n",
    "    if len(words) <= 1:\n",
    "        print(\"sent TOO SHORT:\", sent,\":\")\n",
    "    # print(words)\n",
    "        # return g\n",
    "    # words = sent\n",
    "    n = len(words)\n",
    "    v = len(nts)\n",
    "\n",
    "    out_rules = {}\n",
    "    for rule in binary_rules:\n",
    "        # print(\"Rule_name\", rule[:-1])\n",
    "        out_rules[rule] = 0.0\n",
    "    for rule in unary_rules:\n",
    "        out_rules[rule] = 0.0\n",
    "    counts = out_rules.copy()\n",
    "\n",
    "    #TODO\n",
    "    inside_weights = inside(words, unary_rules, binary_rules, nts,nts2idx, g)\n",
    "    # print(\"LEAVING INSIDE\")\n",
    "    Z = inside_weights.get(0, n, \"S\")\n",
    "\n",
    "   \n",
    "        # return counts, 1\n",
    "   \n",
    "    out_weights = Chart(nts, nts2idx, unary_rules, binary_rules, words)\n",
    "\n",
    "    out_weights.add(0, n, \"S\", value=1)\n",
    "    examined = []\n",
    "    for sub_string_length in reversed(range(2, n+1)): #???\n",
    "        for start_idx in range(n-sub_string_length +1):\n",
    "           # for start_idx in range(n-sub_string_length):\n",
    "            # start_idx +=1\n",
    "            end_idx = start_idx + sub_string_length\n",
    "            # print(\"Chart span\", start_idx, end_idx)\n",
    "\n",
    "            for mid_idx in range(start_idx + 1, end_idx):  # IDX???????\n",
    "                # print(\"subtrees:\", start_idx, mid_idx, end_idx)\n",
    "                examined.append((start_idx, mid_idx, end_idx))\n",
    "                for rule in binary_rules:\n",
    "                    # I think the first must be S\n",
    "                    A, B, C= rule\n",
    "                    # print(\"current prob\", g)\n",
    "                    # g = g.astype(np.float)\n",
    "                    out_rules[rule] += out_weights.get(start_idx, end_idx, A) * inside_weights.get(\n",
    "                        start_idx, mid_idx, B) * inside_weights.get(mid_idx, end_idx, C)\n",
    "                    probB = out_weights.get(\n",
    "                        start_idx, end_idx, A) * g[rule] * inside_weights.get(mid_idx, end_idx, C)\n",
    "                    probC = out_weights.get(\n",
    "                        start_idx, end_idx, A) * g[rule] * inside_weights.get(start_idx, mid_idx, B)\n",
    "                    out_weights.add(start_idx, mid_idx, B, probB)\n",
    "                    out_weights.add(mid_idx, end_idx, C, probC)\n",
    "\n",
    "    # print(\"EXAMINED\", examined)\n",
    "    for k in range(n-1):\n",
    "        relevant_rules = [x for x in unary_rules if words[k] in x]\n",
    "        # print(\"Chart span\", k, k+1)\n",
    "        for rule in relevant_rules:\n",
    "            A, w= rule\n",
    "            # print(rule)\n",
    "            out_rules[rule] += out_weights.get(k, k+1, A)\n",
    "\n",
    "    try:\n",
    "        assert Z > 0.0\n",
    "    except AssertionError:\n",
    "        # TODO - percolate this better\n",
    "        for rule in unary_rules:\n",
    "            counts[rule] = g[rule]\n",
    "            # counts[rule] = 0\n",
    "        for rule in binary_rules:\n",
    "            counts[rule] = g[rule]\n",
    "            # counts[rule] = 0\n",
    "        # print(\"_____\")\n",
    "        print(\"NO TREE FOUND - Z:0\")\n",
    "        print(\"INSIDE\")\n",
    "        print(inside_weights.print())\n",
    "        print(\"OUTSIDE\")\n",
    "        print(out_weights.print())\n",
    "        # COUNT_Z_IS_ZERO +=1\n",
    "        print(words)\n",
    "        return counts, Z\n",
    "\n",
    "\n",
    "    for rule in unary_rules:\n",
    "        counts[rule] = (1/Z) * out_rules[rule] * g[rule]\n",
    "    for rule in binary_rules:\n",
    "        counts[rule] = (1/Z) *out_rules[rule] * g[rule]\n",
    "        \n",
    "    return counts, Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_rules_starting_with(A, keys):\n",
    "    # keys = list(rules.keys())\n",
    "    A_keys = [key for key in keys if key[0] == A]\n",
    "    if len(A_keys) == 0:\n",
    "        return []\n",
    "    return A_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(rules, sents, nts, nts2idx, G):\n",
    "    # expected_counts = outside()\n",
    "    unary_rules = rules[0]\n",
    "    binary_rules = rules[1]\n",
    "\n",
    "    print(\"calculating expected counts...\")\n",
    "    expected_counts_vec = []\n",
    "    Zs = []\n",
    "    progress_bar = tqdm(sents)\n",
    "    for sent in sents:\n",
    "        # print(sent.split())\n",
    "        if len(sent.split()) <= 2:\n",
    "            continue\n",
    "        count, Z = outside(sent, unary_rules, binary_rules, nts, nts2idx, G)\n",
    "        expected_counts_vec.append(count)\n",
    "        Zs.append(Z)\n",
    "        progress_bar.update()\n",
    "    progress_bar.close()\n",
    "    \n",
    "    avg_likelihood = np.mean(Zs)\n",
    "    Zs = [Z for Z in Zs if Z >0.0]\n",
    "    avg_log_likelihood = np.log(Zs)\n",
    "    var = np.std(avg_log_likelihood)\n",
    "    avg_log_likelihood = np.mean(avg_log_likelihood)\n",
    "    # print(\"avg_likelihood\", avg_likelihood)\n",
    "    print(\"avg_log_likelihood?\", avg_log_likelihood)\n",
    "    print(\"var\", var)\n",
    "\n",
    "\n",
    "    # likelihood = get_likelihood(expected_counts_vec, unary_rules, binary_rules, G)\n",
    "    # print(\"new likelihood\", likelihood) # I do think its log likelihood...\n",
    "    return expected_counts_vec, avg_log_likelihood\n",
    "\n",
    "    def e_step2(rules, sents, nts, nts2idx, G):\n",
    "    # expected_counts = outside()\n",
    "        unary_rules = rules[0]\n",
    "        binary_rules = rules[1]\n",
    "\n",
    "        print(\"calculating expected counts...\")\n",
    "        expected_counts_vec = []\n",
    "        Zs = []\n",
    "        progress_bar = tqdm(sents)\n",
    "        for sent in sents:\n",
    "            # print(sent.split())\n",
    "            if len(sent.split()) <= 2:\n",
    "                continue\n",
    "            count, Z = outside(sent, unary_rules, binary_rules, nts, nts2idx, G)\n",
    "            expected_counts_vec.append(count)\n",
    "            Zs.append(Z)\n",
    "            progress_bar.update()\n",
    "        progress_bar.close()\n",
    "        \n",
    "        avg_likelihood = np.mean(Zs)\n",
    "        Zs = [Z for Z in Zs if Z >0.0]\n",
    "        avg_log_likelihood = np.log(Zs)\n",
    "        var = np.std(avg_log_likelihood)\n",
    "        avg_log_likelihood = np.mean(avg_log_likelihood)\n",
    "        # print(\"avg_likelihood\", avg_likelihood)\n",
    "        # print(\"avg_log_likelihood?\", avg_log_likelihood)\n",
    "        # print(\"var\", var)\n",
    "\n",
    "\n",
    "        log_likelihood = get_likelihood(expected_counts_vec, unary_rules, binary_rules, G)\n",
    "        print(\"likelihood\", likelihood) \n",
    "        return expected_counts_vec, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def m_step(expected_counts_vec, nts, rules):\n",
    "    # print(\"Example exp.counts\", expected_counts_vec)\n",
    "    print(\"M Step\")\n",
    "\n",
    "    \n",
    "    unary_rules = rules[0]\n",
    "    binary_rules = rules[1]\n",
    "    thetas = {}\n",
    "    sum_occurrences_of_A = {}\n",
    "\n",
    "    # print(\"building dict summing all occurrences of A\")\n",
    "    # print(nts)\n",
    "    for nt in nts:\n",
    "        un_counts = 0\n",
    "        bin_counts = 0\n",
    "   \n",
    "        A_rules = get_list_of_rules_starting_with(nt,unary_rules)\n",
    "        # print(\"U\", len(A_rules))\n",
    "        if A_rules:\n",
    "            for rule in A_rules:\n",
    "                un_counts += np.sum([count[rule] for count in expected_counts_vec]) \n",
    "            # print(summed_counts)\n",
    "        \n",
    "        A_rules = get_list_of_rules_starting_with(nt,binary_rules)\n",
    "        # print(\"B\", len(A_rules))\n",
    "        if A_rules:\n",
    "            for rule in A_rules:\n",
    "                bin_counts += np.sum([count[rule] for count in expected_counts_vec]) \n",
    "            \n",
    "        sum_occurrences_of_A[nt] = bin_counts + un_counts\n",
    "        if sum_occurrences_of_A[nt] == 0.0:\n",
    "            print(\"ZERO\", nt)\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"Calculating unary theta rules...\")\n",
    "    for rule in unary_rules:\n",
    "        A,w = rule\n",
    "        summed_count = np.sum([count[rule] for count in expected_counts_vec]) \n",
    "\n",
    "        # assert sum_occurrences_of_A[A] > 0.0\n",
    "        # assert summed_count <= sum_occurrences_of_A[A]\n",
    "        if sum_occurrences_of_A[A] > 0.0:\n",
    "            thetas[rule] =  np.exp((summed_count / sum_occurrences_of_A[A]))\n",
    "            # print(\"sum occurrences\",sum_occurrences_of_A[A] )\n",
    "            # print(\"nonzero rule\", rule, thetas[rule])\n",
    "        else:\n",
    "            thetas[rule] =  0.0\n",
    "    # except:\n",
    "        # thetas[rule] =  0.0\n",
    "            \n",
    "    # print(\"Calculating binary theta rules...\")\n",
    "    for rule in binary_rules:\n",
    "\n",
    "        A,B,C = rule\n",
    "        summed_count = sum([count[rule] for count in expected_counts_vec]) \n",
    "       \n",
    "        # assert summed_count <= sum_occurrences_of_A[A]\n",
    "        if sum_occurrences_of_A[A] > 0.0:\n",
    "            thetas[rule] =  np.exp((summed_count / sum_occurrences_of_A[rule[0]]))\n",
    "            # print(\"sum occurrences\",sum_occurrences_of_A[A] )\n",
    "            # print(\"nonzero rule\",rule, thetas[rule])\n",
    "        else:\n",
    "            thetas[rule] =  0.0\n",
    "\n",
    "    return thetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(sents, rules, nts, nts2idx, G, max_iter=10):\n",
    "    THRESHOLD = .01\n",
    "    THRESHOLD = .1\n",
    "    MIN_THETA = .0001\n",
    "    change = 1\n",
    "    avg_loglikelihoods = []\n",
    "    iters = 1\n",
    "    g = G\n",
    "    thetas = []\n",
    "    avg_likelihoods = []\n",
    "\n",
    "    unary_rules, binary_rules = rules\n",
    "   \n",
    "    # rules = [unary_tuples, binary_tuples]\n",
    "\n",
    "    while True:\n",
    "        print(\"ITER\", iters)\n",
    "        # _, loglikelihoods = e_step(rules, sents)\n",
    "        expected_counts_vec, avg_likelihood = e_step(rules, sents, nts, nts2idx, g)\n",
    "\n",
    "        avg_likelihoods.append(avg_likelihood)\n",
    "        if len(avg_likelihoods) > 2:\n",
    "            print(\"DIFF\", abs(avg_likelihoods[-1] - avg_likelihoods[-2]))\n",
    "\n",
    "        if len(avg_likelihoods) > 2 and abs(avg_likelihoods[-1] - avg_likelihoods[-2]) < THRESHOLD:\n",
    "            print(\"NO IMPROVEMENT\")\n",
    "            break\n",
    "        elif iters >max_iter:\n",
    "            break\n",
    "        # E step:\n",
    "        # u_thetas, b_thetas = m_step(expected_counts_vec, rules)\n",
    "        thetas = m_step(expected_counts_vec, nts, rules)\n",
    "        # g = thetas\n",
    "\n",
    "        # TODO - flex g\n",
    "        for rule in unary_rules:\n",
    "            # these if statements are unnecesary\n",
    "            if thetas[rule] < MIN_THETA:\n",
    "                g[rule] = np.exp(thetas[rule])\n",
    "            else:\n",
    "                g[rule] = np.exp(thetas[rule])\n",
    "        for rule in binary_rules:\n",
    "            if thetas[rule] < MIN_THETA:\n",
    "                g[rule] = np.exp(thetas[rule])\n",
    "            else:\n",
    "                g[rule] = np.exp(thetas[rule])\n",
    "\n",
    "        # break\n",
    "        iters += 1 \n",
    "        \n",
    "    print(\"EXITING after\", iters, \"iters of e_step\")\n",
    "    return g, thetas, avg_likelihoods[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path_to_sents, path_to_pcfg):\n",
    "    \n",
    "    train_sents = load_text(path_to_sents)\n",
    "    unary_rules, binary_rules, nts = load_rules_from_pcfg(path_to_pcfg)\n",
    "\n",
    "    print(unary_rules[0])\n",
    "    print(binary_rules[0])\n",
    "    print(nts)\n",
    "    print(\"len(binary_rules)\", len(binary_rules))\n",
    "    print(\"len(unary_rules)\", len(unary_rules))\n",
    "    print(\"len rules\", len(unary_rules) +  len(binary_rules))\n",
    "\n",
    "    nts2idx = {}\n",
    "    for i, nt in enumerate(nts):\n",
    "        nts2idx[nt] = i\n",
    "    \n",
    "    G = {}\n",
    "    binary_tuples = []\n",
    "    for rule in binary_rules:\n",
    "        A, B, C, g = rule\n",
    "        key = (A,B,C)\n",
    "        G[key] = g.astype(np.float)\n",
    "        binary_tuples.append(key)\n",
    "    unary_tuples = []\n",
    "    for rule in unary_rules:\n",
    "        A, w, g = rule\n",
    "        key = (A,w)\n",
    "        G[key] = g.astype(float)\n",
    "        unary_tuples.append(key)\n",
    "    rules = [unary_tuples, binary_tuples]\n",
    "    # print(G)\n",
    "\n",
    "\n",
    "    \n",
    "    g, t, final_ll = EM(train_sents, rules, nts, nts2idx, G)\n",
    "    return g,t, final_ll\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(expected_counts_vec, unary_rules, binary_rules, g):\n",
    "    likelihood = 0 # shape \n",
    "    for count in expected_counts_vec: \n",
    "        for rule in unary_rules:\n",
    "            likelihood += count[rule] * np.log(g[rule])\n",
    "        for rule in binary_rules:\n",
    "            likelihood += count[rule] * np.log(g[rule])\n",
    "    return likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_likelihood(expected_counts_vec, unary_rules, binary_rules, g):\n",
    "#     likelihood = 0 # shape \n",
    "#     # for count in expected_counts_vec: \n",
    "#     for rule in unary_rules:\n",
    "#         count = np.sum([count[rule] for count in expected_counts_vec])\n",
    "#         likelihood += count * np.log(g[rule])\n",
    "#     for rule in binary_rules:\n",
    "#         count = np.sum([count[rule] for count in expected_counts_vec])\n",
    "#         likelihood += count * np.log(g[rule])\n",
    "#     return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_tree():\n",
    "\n",
    "def test(path_to_sents, path_to_pcfg, path_to_dict):\n",
    "    test_sents = load_text(path_to_sents)\n",
    "    unary_rules, binary_rules, nts = load_rules_from_pcfg(path_to_pcfg)\n",
    "   \n",
    "    print(unary_rules[0])\n",
    "    print(binary_rules[0])\n",
    "    print(nts)\n",
    "    print(\"len(binary_rules)\", len(binary_rules))\n",
    "    print(\"len(unary_rules)\", len(unary_rules))\n",
    "\n",
    "    nts2idx = {}\n",
    "    for i, nt in enumerate(nts):\n",
    "        nts2idx[nt] = i\n",
    "\n",
    "    G_str = np.load(path_to_dict, allow_pickle=True)\n",
    "    G_str = G_str[()]\n",
    "    print(type(G_str))\n",
    "    # print(G.items())\n",
    "    print(\"G\",len(G_str.items()),\"R\", len(unary_rules) + len(binary_rules))\n",
    "    assert len(G_str.items()) == len(unary_rules) + len(binary_rules)\n",
    "    G = {}\n",
    "    binary_tuples = []\n",
    "    for rule in binary_rules:\n",
    "        A, B, C, g = rule\n",
    "        key = (A,B,C)\n",
    "        g = G_str[key]\n",
    "        G[key] = g.astype(np.float)\n",
    "        binary_tuples.append(key)\n",
    "    unary_tuples = []\n",
    "    for rule in unary_rules:\n",
    "        A, w, g = rule\n",
    "        key = (A,w)\n",
    "        g = G_str[key]\n",
    "        G[key] = g.astype(float)\n",
    "        unary_tuples.append(key)\n",
    "    rules = [unary_tuples, binary_tuples]\n",
    "\n",
    "\n",
    "    expected_counts_vec, avg_likelihood = e_step2(rules, test_sents, nts, nts2idx, G)\n",
    "\n",
    "\n",
    "    return expected_counts_vec, avg_likelihood\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    author_names = [\"Alice Mabel\", \"Ayn Rand\", \"Bram Stoker\", \"Charles Dickens\", \"Emily Bronte\", \"Frederick Douglas\", \"Herman Hesse\", \"Herman Melville\", \"Jane Austen\", \"Leo Tolstoy\", \"Lewis Carroll\" , \"Mary Shelly Wollstonecraft\", \"Scott Fitzgerald\", \"Migel de Cervantes\", \"Ulysses\"]\n",
    "    lls = []\n",
    "    for i, name in enumerate(author_names):\n",
    "        print(\"TRAINING\" , name, i)\n",
    "        path_to_sents = \"../make-data/train_sentences_10/training_author_idx_\" + i + \".txt\"\n",
    "        path_to_pcfg =  \"../make-data/pcfg_ug_flat_7000.txt\"\n",
    "        # path_to_dict= \"../make-data/output_10_7000/g_model_author_idx_\"+ i  + \".txt\"\n",
    "        # vec, ll = train(path_to_sents, path_to_pcfg, path_to_dict)\n",
    "        print(ll)\n",
    "        # lls.append(ll)\n",
    "        # np.save(path_to_output_dir + \"g_model_author_idx_\" + str(i) , G)\n",
    "        # np.save(path_to_output_dir + \"t_model_author_idx_\" + str(i) , T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_names = [\"Alice Mabel\", \"Ayn Rand\", \"Bram Stoker\", \"Charles Dickens\", \"Emily Bronte\", \"Frederick Douglas\", \"Herman Hesse\", \"Herman Melville\", \"Jane Austen\", \"Leo Tolstoy\", \"Lewis Carroll\" , \"Mary Shelly Wollstonecraft\", \"Scott Fitzgerald\", \"Migel de Cervantes\", \"James Joyce\"]\n",
    "\n",
    "# Need some file names in here\n",
    "for a, author in enumerate(author_names):\n",
    "    print(\"TRAINING\" , a, author )\n",
    "    \n",
    "    path_to_sents = \"\" + str(a) + \".txt\"\n",
    "    path_to_pcfg =  \"\"\n",
    "    path_to_output_dir = \"\"\n",
    "    G, T, ll = train(path_to_sents, path_to_pcfg)\n",
    "    print(ll)\n",
    "\n",
    "    np.save(path_to_output_dir + \"g_model_author_idx_\" + str(a) , G)\n",
    "    np.save(path_to_output_dir + \"t_model_author_idx_\" + str(a) , T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_names = [\"Alice Mabel\", \"Ayn Rand\", \"Bram Stoker\", \"Charles Dickens\", \"Emily Bronte\", \"Frederick Douglas\", \"Herman Hesse\", \"Herman Melville\", \"Jane Austen\", \"Leo Tolstoy\", \"Lewis Carroll\" , \"Mary Shelly Wollstonecraft\", \"Scott Fitzgerald\", \"Migel de Cervantes\", \"James Joyces\"]\n",
    "\n",
    "\n",
    "# Need some file names in here\n",
    "lls = []\n",
    "vecs = []\n",
    "for a, author in enumerate(author_names):\n",
    "    print(\"TESTING\" , a, author)\n",
    "    path_to_sents = '' + str(i) + \".txt\"\n",
    "    path_to_pcfg =  ''\n",
    "    path_to_dict= \" \" + str(a)  + \".npy\"\n",
    "    vec , ll = test(path_to_sents, path_to_pcfg, path_to_dict)\n",
    "    lls.append(ll)\n",
    "    print(ll)\n",
    "\n",
    "np.save(\"\", lls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
